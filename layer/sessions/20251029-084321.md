# Session: review persona capture and design
**ID**: 20251029-084321
**Started**: 2025-10-29T12:43:21Z
**LLM**: claude
**Git Branch**: neuro-symbolic-knowledge-system
**Session Tag**: session-20251029-084321-start
**Starting Commit**: dcaea0c9f70cada200ed08c6f8fb524124f18446

## Previous Session Context
Previous session established the three-layer persona architecture: observations extracted from sessions (Layer 1), beliefs stored with evidence links (Layer 2), and intelligent LLM-driven persona sessions (Layer 3). Created unified `schema.sql` combining observation and belief tables, established `.patina/db/` for regeneratable databases (code.db for scraping, facts.db for sessions/beliefs), and built `/persona-start` and `/persona-end` commands. Key insight: projects define reality, personas adapt with exceptions. Loaded 7 sessions with 11 patterns, 7 technologies, and 5 decisions into working facts.db.

## Goals
- [ ] review persona capture and design

## Activity Log
### 08:43 - Session Start
Session initialized with goal: review persona capture and design
Working on branch: neuro-symbolic-knowledge-system
Tagged as: session-20251029-084321-start

### 08:45 - System Review & Database Cleanup
Explored current persona system architecture (3 layers):
- Layer 1: Observations (7 sessions, 11 patterns, 7 technologies, 5 decisions in facts.db)
- Layer 2: Beliefs (22 beliefs with confidence scores)
- Layer 3: Intelligent Agent (bash scripts + Claude orchestration)

Cleaned up old unused databases (3.5MB freed):
- Removed: code_intelligence.db, knowledge.db, knowledge_code.db, knowledge-recode-v2.db
- Current: .patina/db/code.db (2.2M), .patina/db/facts.db (184K)

### 09:00 - Evidence Linking Fix
**Problem Identified:** 22 beliefs existed but `belief_observations` table was empty - no citations linking beliefs to supporting evidence.

**Root Cause:** `persona-start.sh` workflow gap - instructed to "codify with citations" but no concrete SQL examples provided.

**Solution (both parts completed):**

1. **Retroactive Linking** - Created `.patina/link-belief-evidence.sql`:
   - Linked 22 existing beliefs to supporting patterns/decisions/technologies
   - 39 evidence citations created
   - 19 of 22 beliefs now have evidence links (86%)
   - 3 beliefs need more evidence (expected - only 7 of 227 sessions loaded)

2. **Workflow Fix** - Updated `.claude/bin/persona-start.sh`:
   - Added "CRITICAL: Evidence Linking" section with SQL examples
   - Step-by-step workflow: query evidence → insert belief → link citations
   - Documents observation_type ('pattern', 'technology', 'decision', 'challenge')
   - Documents validates field (1=supports, 0=contradicts)
   - Future persona sessions will properly link evidence

**Verification:**
```
Belief: never_commit_secrets_to_disk (0.95 confidence)
Evidence:
  ├─ pattern: tmpfs-for-secrets (20251008-061520) [supports]
  ├─ pattern: credential-management (20251007-210232) [supports]
  └─ pattern: security-review-generated-code (20251007-185647) [supports]
```

**Statistics:** 22 beliefs, 19 with citations (86%), 39 total evidence links

### 09:30 - Confidence Scoring Enforcement with Prolog

**Key Insight:** Prolog is the guard rail system - it keeps LLM from making arbitrary confidence decisions.

**Problem:** Currently confidence is set manually during persona sessions. LLM estimates values (0.80-0.95) based on judgment, not rules. This is subjective and non-repeatable.

**Solution:** Created `.patina/confidence-rules.pl` - deterministic rules that LLM MUST obey.

**Architecture:**
- **Prolog defines HOW** confidence changes (the rules)
- **SQLite stores WHAT** exists (beliefs, evidence)
- **LLM orchestrates WHEN** to apply rules (dialogue, extraction)
- **But LLM cannot decide confidence** - only Prolog can

**Rules Implemented:**
```prolog
% Initial confidence based on evidence count
query_initial_confidence(3, 0.8).           % 3 evidence → 0.8
query_initial_confidence(1, 0.65).          % 1 evidence → 0.65

% Adjusting confidence with new evidence
strengthen_confidence(0.75, 2, 0.95).       % +2 supporting → +0.2 (hit max 0.95)
adjust_confidence(0.80, 2, 1, 0.9).         % +2 support, -1 contradict = net +0.1

% Bounds
max_confidence(0.95).  % Always leave room for doubt
min_confidence(0.3).   % Keep deprecated beliefs discoverable
```

**Tested and working:**
```bash
$ scryer-prolog confidence-rules.pl -g "query_initial_confidence(3, C), write(C), halt."
0.8

$ scryer-prolog confidence-rules.pl -g "strengthen_confidence(0.75, 2, C), write(C), halt."
0.95

$ scryer-prolog confidence-rules.pl -g "adjust_confidence(0.80, 2, 1, C), write(C), halt."
0.9
```

**Updated Workflow in `persona-start.sh`:**
1. **Count evidence** (SQLite query)
2. **Query Prolog** for confidence value (deterministic)
3. **Insert belief** with Prolog-determined confidence
4. **Link evidence** (SQLite inserts)

**Philosophy:** "YOU CANNOT OVERRIDE THESE RULES. If you think a belief should have different confidence, the rules are wrong and need to be updated, not bypassed."

**Files Created:**
- `.patina/confidence-rules.pl` - Confidence scoring rules (Prolog)
- Updated `.claude/bin/persona-start.sh` - Enforces Prolog consultation

### 10:30 - Neuro-Symbolic Architecture Critical Analysis

**Question:** Is the neuro-symbolic architecture (SQLite + Prolog + LLM) sound? Are we missing critical pieces?

**Context:** Realized Patina project beliefs are separate from future universal persona app. Current scope: 245 sessions about building Patina → beliefs about Patina development only.

**ML/Neuro-Symbolic Expert Analysis:**

**What's Right:**
- ✅ Neuro-symbolic split (LLM extracts, Prolog reasons, SQLite stores)
- ✅ Confidence as continuous value (0.0-1.0)
- ✅ Evidence provenance (`belief_observations` table)

**Critical Gaps Identified:**

1. **No Semantic Search/Embeddings**
   - Current: Keyword matching only (`WHERE pattern_name = 'X'`)
   - Problem: Misses semantic similarity ("code audit" ≠ "security review")
   - Solution: Add sqlite-vss extension for vector similarity search
   - Impact: Enables cross-domain concept matching, better retrieval

2. **No Belief Relationship Graph**
   - Current: Flat belief list with `parent_belief_id` (linear only)
   - Problem: Beliefs form a graph (supports/contradicts/correlates)
   - Solution: Add `belief_relationships` table
   - Impact: Transitive reasoning, conflict detection, cross-domain patterns

3. **No Temporal Dynamics**
   - Current: Single confidence value, `last_validated` field
   - Problem: No belief evolution tracking, no temporal decay
   - Solution: Add `belief_history` table
   - Impact: Show how beliefs change over time, context windows

4. **No Retrieval Layer**
   - Current: Undefined how LLM queries beliefs
   - Problem: Can't dump all beliefs (context limit), need smart retrieval
   - Solution: Hybrid retrieval (semantic search + Prolog reasoning + ranking)
   - Impact: LLM gets relevant beliefs with provenance

5. **Domain Organization Unclear**
   - Question: Flat tags or hierarchical ontology?
   - Question: How do cross-domain beliefs work?
   - Question: Universal vs domain-specific vs project-specific?
   - Needs: Design decision on domain structure

**Architecture Decisions Required:**

1. **Is Prolog earning its keep?**
   - Currently: Confidence rules only
   - Should: Relationship graph, transitive queries, conflict detection
   - Or: Replace with Python if not doing complex reasoning

2. **Global Persona vs Project Beliefs?**
   - Option A: Global persona.db + project views (recommended)
   - Option B: Federated (each project separate)
   - Need: Decision on architecture

3. **Embedding Strategy?**
   - Option A: sqlite-vss extension (recommended - simple)
   - Option B: Separate vector DB (Qdrant/Chroma - more complex)
   - Option C: PostgreSQL + pgvector (overkill for single-user)

**Implementation Roadmap:**

Phase 1: Core Fixes (1-2 weeks)
- Add belief_relationships table
- Add belief_history table
- Implement Prolog relationship queries

Phase 2: Semantic Search (1-2 weeks)
- Install sqlite-vss extension
- Generate embeddings for beliefs/observations
- Build embedding pipeline

Phase 3: Retrieval Layer (2-3 weeks)
- Hybrid retrieval API
- Ranking algorithm
- Prompt construction

**Key Insight:** Foundation is solid, but system can't function without:
1. Embeddings (semantic search)
2. Belief relationships (graph reasoning)
3. Retrieval layer (how LLM uses beliefs)

**Documentation:** Complete analysis in `layer/surface/neuro-symbolic-architecture-critique.md`


### 08:19 - Update (covering since 08:43)

**Git Activity:**
- Commits this session: 0
- Files changed: 17
- Last commit: 6 days ago

**Work Completed:**
- Reviewed persona capture system (layer/personas, layer/sessions)
- Explored current 3-layer architecture implementation status
- Cleaned up old unused databases (3.5MB freed)
- Fixed evidence linking gap (retroactive + workflow enforcement)
- Built confidence scoring rules in Prolog (deterministic, tested)
- Conducted ML/neuro-symbolic expert analysis of architecture
- Created comprehensive critique document with implementation roadmap
- Discussed observation extraction strategies (245 sessions, 7 extracted)
- Clarified project scope (Patina project beliefs vs universal persona app)
- Analyzed RLSF paper - validates symbolic feedback approach

**Key Decisions:**
- Evidence linking was broken (beliefs had no citations) - fixed with retroactive SQL script + updated persona-start.sh workflow
- Confidence scoring moved from LLM judgment to Prolog rules enforcement ("you cannot override these rules")
- Identified 5 critical gaps: semantic search, belief graph, temporal dynamics, retrieval layer, domain organization
- Decided hybrid approach (embeddings + Prolog) is necessary for cross-domain persona
- sqlite-vss recommended over separate vector DB (maintains simplicity)
- Observation extraction: incremental during persona sessions (not bulk LLM job)
- RLSF paper validates using Prolog as enforcer, not just query engine

**Challenges & Solutions:**
- Challenge: Understanding timeline (245 MD files) vs actionable knowledge (belief system)
- Solution: Timeline as evidence base, extract recent sessions first, archive consulted on-demand
- Challenge: Belief system scope confusion (project vs universal persona)
- Solution: Clarified - current work is Patina project beliefs only, universal app is future
- Challenge: Whether embeddings are necessary for knowledge system
- Solution: Yes for semantic search and cross-domain matching, but Prolog still critical for logic

**Patterns Observed:**
- Neuro-symbolic architecture requires clear separation: LLM extracts, Prolog validates, SQLite stores
- Token-level feedback (RLSF paper) > scalar feedback - precise corrections learn faster
- Symbolic systems as enforcers not just databases - guard rails for LLM behavior
- Runtime validation often sufficient (no fine-tuning needed for correctness checks)
- Evidence linking critical for explainability - "why do I believe X?" needs citations
- Confidence as continuous value (0.0-1.0) enables Bayesian-ish belief revision


## Session Classification
- Work Type: exploration
- Files Changed:        0
- Commits:        0
- Patterns Modified:        0
- Session Tags: session-20251029-084321-start..session-20251029-084321-end
