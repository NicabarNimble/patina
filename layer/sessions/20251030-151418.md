# Session: embeddings-integration-roadmap
**ID**: 20251030-151418
**Started**: 2025-10-30T19:14:18Z
**LLM**: claude
**Git Branch**: neuro-symbolic-knowledge-system
**Session Tag**: session-20251030-151418-start
**Starting Commit**: 9e3ab0a354aebfc3d0e34a17494f5c3660874329

## Previous Session Context
Created comprehensive 4-week embeddings integration roadmap with CoreML-first approach for on-device semantic search using sqlite-vss. Merged neuro-symbolic persona architecture (PR #39) to main. Designed hybrid retrieval combining embeddings for semantic discovery with Prolog for logical reasoning across 227+ session observations.

## Goals
- [ ] embeddings-integration-roadmap

## Activity Log
### 15:14 - Session Start
Session initialized with goal: embeddings-integration-roadmap
Working on branch: neuro-symbolic-knowledge-system
Tagged as: session-20251030-151418-start


### 21:24 - Update (covering since 15:14)

**Git Activity:**
- Commits this session:        0
- Files changed: 3
- Last commit: 10 hours ago

**Work completed:**
- Conducted comprehensive research on embedding approaches for Apple Silicon (Candle, rust-bert, MLX, swift-embeddings, CoreML)
- Identified critical cross-platform requirement: need Linux search support (not just Mac)
- Discovered rust-bert uses Rosetta 2 on Apple Silicon (slow, not native ARM)
- Discovered Candle has Metal stability issues and 2x slower than Python
- Discovered MLX/swift-embeddings are Apple-only (cannot query from Linux)
- Found pre-converted ONNX models on HuggingFace (no Python export needed!)
- Completely rewrote embeddings-integration-roadmap.md for ONNX Runtime approach
- Updated roadmap from CoreML-first (v2) → ONNX Runtime (v3)
- Removed Python export scripts, Swift embedding helper, multi-language stack
- Added ONNX model download instructions, pure Rust implementation, cross-platform support

**Key decisions:**
- **ONNX Runtime over CoreML**: Cross-platform (Mac+Linux) > Mac-only performance
- **Same vector space everywhere**: Critical for Linux queries to work against Mac-generated embeddings
- **Pure Rust at runtime**: Use `ort` crate (production-proven, Twitter uses it for 100M+ users)
- **Pre-converted models**: Download `.onnx` files directly, avoid Python dependency entirely
- **HuggingFace tokenizers**: Rust tokenizers crate (no Python needed)
- **Accept slower speed**: 30-50ms (ONNX Metal) vs 20ms (CoreML ANE) is acceptable trade-off

**Challenges faced:**
- Initial CoreML approach looked simple but was Mac-only
- Discovered vector space incompatibility: CoreML embeddings ≠ Linux queries
- Each framework research revealed limitations (Rosetta, Metal bugs, platform locks)
- Had to balance performance (CoreML fastest) vs portability (ONNX cross-platform)
- User constraint: avoid Python (eliminates simple subprocess approach)

**Patterns observed:**
- Platform-specific ML frameworks (CoreML, MLX) create vendor lock-in
- Cross-platform requirement changes architecture fundamentally
- ONNX is the industry standard for model portability
- Pre-converted models eliminate Python dependency elegantly
- Twitter's production use of `ort` crate is strong validation
- Model file format determines where you can search, not just where you generate


## Session Classification
- Work Type: pattern-work
- Files Changed:        1
- Commits:        1
- Patterns Modified:        1
- Session Tags: session-20251030-151418-start..session-20251030-151418-end
