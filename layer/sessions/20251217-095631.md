# Session: Patina
**ID**: 20251217-095631
**Started**: 2025-12-17T14:56:31Z
**LLM**: claude
**Git Branch**: patina
**Session Tag**: session-20251217-095631-start
**Starting Commit**: d04c51366cb6df55ef514df4b9c22883fdd6cf3c

## Previous Session Context
Last session reviewed Apple's CLaRa paper (continuous latent reasoning for RAG) against Patina's Phase 3 feedback loop design. Key validation: CLaRa confirms downstream task completion (commits for us, generation quality for them) is the right supervision signal for retrieval - same philosophy, different scale. Identified potential enhancements: query expansion via session context logging, compression as filtering for utility boosting. No code written - pure research session comparing external RAG research to our architectural decisions.

## Goals
- [x] Review REFRAG paper (Rethinking RAG-based Decoding)
- [x] Analyze against Patina core values and feedback loop spec
- [x] Identify potential spec enhancements from research insights

## Activity Log
### 09:56 - Session Start
Session initialized with goal: Patina
Working on branch: patina
Tagged as: session-20251217-095631-start


### 10:02 - Update (covering since 09:56)

**Git Activity:**
- Commits this session:        0
- Files changed: 1
- Last commit: 69 minutes ago

**Work Completed:**
- Anchored in layer/core values (dependable-rust, unix-philosophy, adapter-pattern)
- Read spec-feedback-loop.md for review context
- Analyzed Meta REFRAG paper (arXiv:2509.01092v2) on RAG decoding optimization

**REFRAG Paper Analysis:**

The paper addresses inefficiency in RAG decoding by observing that:
1. RAG attention is block-diagonal - retrieved passages have near-zero cross-attention
2. Most context computation is wasteful - only a small subset directly relevant
3. Compression can be selective - RL policy learns which chunks expand vs compress

Key result: 30.85× TTFT acceleration without perplexity loss (k=32 compression).

**Key Connections to Our Spec:**
| REFRAG Concept | Patina Analog |
|----------------|---------------|
| Block-diagonal attention | core/surface/dust layers (files in different layers have different utility) |
| Selective compression via RL | Utility scoring from feedback loop ("what helps generation" ≈ "what gets committed") |
| Pre-computed chunk embeddings | Git-derived projections cached in SQLite |
| Compression rate k hyperparameter | Oracle weights α,β,γ,δ (same meta-optimization problem) |

**Key Insight Validated:**
REFRAG's Figure 7 (attention heatmaps) empirically shows retrieved passages are semantically independent - computation across unrelated chunks is wasteful. This validates our **Stability + Utility = Relevance** principle:
- High-utility files deserve "full attention" (expansion)
- Low-utility files can be compressed/deprioritized

**Potential Enhancements Identified:**

1. **Variable-Depth Retrieval by Utility**
   - High-utility file (hit_rate > 0.6): Return full content
   - Medium-utility: Return summary + key symbols
   - Low-utility: Return path only or exclude
   - Reduces context length while preserving signal

2. **Utility-Based Presentation Policy**
   - Extend doc_utility with `presentation_depth` column
   - Derive from hit_rate: 'full' / 'summary' / 'minimal'
   - Track if presentation depth correlates with outcomes

3. **Query Context → Selective Expansion**
   - Log session goals, prior queries, query-file similarity
   - Derive which files need full context vs compressed
   - Future "context compression" phase after feedback loop works

**Key Difference from Patina:**
| Aspect | REFRAG | Patina |
|--------|--------|--------|
| Focus | LLM inference optimization | Retrieval quality measurement |
| Requires | Model training/fine-tuning | Off-the-shelf LLM |
| Signal | Perplexity during decoding | Commit correlation post-hoc |
| Approach | Compress at decoder level | Instrument at retrieval level |

**Verdict: Relevant but Complementary**
- REFRAG answers "how should we present context to the LLM?"
- Patina answers "which context should we present?"
- Both use downstream task completion as supervision signal
- Block-diagonal attention validates utility-based prioritization
- RL selective compression validates same philosophy as our utility scoring
- However, REFRAG requires LLM training (outside our "no training" approach)

**Synthesis with Previous CLaRa Review:**
Both CLaRa and REFRAG validate our core design principle:
- CLaRa: Joint optimization via gradient flow (generation loss → retriever)
- REFRAG: Selective compression via RL (prediction quality → expansion policy)
- Patina: Heuristic tuning via commit correlation (commit data → scry feedback)
Same philosophy, different implementation scales. All use task completion as supervision.


### 10:03 - Update (covering since 10:02)

**Git Activity:**
- Commits this session:        0
- Files changed: 1
- Last commit: 71 minutes ago


## Session Classification
- Work Type: exploration
- Files Changed:        0
- Commits:        0
- Patterns Modified:        0
- Session Tags: session-20251217-095631-start..session-20251217-095631-end
