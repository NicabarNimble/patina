# Stable Parsing Engine - Design Document

## What We Built

A semantic code analysis engine that transforms codebases into queryable knowledge for LLMs.

**The Problem**: LLMs waste tokens reading entire files when they only need specific context.

**Our Solution**: Extract semantic meaning from code and store it in a queryable database.

## Architecture

```
Your Code â†’ Tree-sitter â†’ Semantic Extraction â†’ DuckDB â†’ LLM Context
```

### Components

```
src/semantic/
â”œâ”€â”€ extractor/
â”‚   â”œâ”€â”€ ast_processor.rs    # Coordinates AST extraction
â”‚   â”œâ”€â”€ call_graph.rs       # Function relationships
â”‚   â””â”€â”€ documentation.rs    # Doc comments & keywords
â””â”€â”€ store/
    â”œâ”€â”€ mod.rs              # KnowledgeStore trait
    â””â”€â”€ duckdb.rs           # Database implementation
```

### Data Flow

1. **Parse**: Tree-sitter generates AST from source files
2. **Extract**: Pull out functions, types, docs, call relationships
3. **Store**: Save in DuckDB with 10 specialized tables
4. **Query**: Use SQL or trait methods to retrieve context

## Database Schema

```sql
-- Core tables (simplified view)
function_facts      # Function metadata (async, unsafe, params, returns)
documentation       # Searchable docs with keywords array
call_graph         # Who calls whom, where
type_vocabulary    # Structs, traits, enums
behavioral_hints   # Unwrap, panic, unsafe usage
imports            # Dependencies
```

## Current Commands

### `patina scrape` - Build the Knowledge Base
```bash
patina scrape --init          # Initialize database
patina scrape                 # Extract from current directory
patina scrape --force         # Full re-index
patina scrape --repo dagger   # Scrape reference repo
```

## Next: Context Retrieval

### What We Have
The `KnowledgeStore` trait already provides query methods:
- `query_by_keywords()` - Find symbols by keyword
- `get_call_chain()` - Recursive call graph traversal  
- `get_documentation()` - Retrieve docs
- `get_function_facts()` - Get function metadata

### What We Need to Build

#### 1. Context Command
```bash
patina context "How does authentication work?"
```

#### 2. Pipeline Components

```rust
// Keyword extraction from natural language
KeywordExtractor::extract("How does auth work?") -> ["auth"]

// Context assembly using existing store methods
ContextAssembler::build(keywords) -> Context {
    symbols: Vec<Symbol>,
    relationships: Vec<CallRelation>,
    relevance_scores: Vec<f32>
}

// Format for specific LLMs
ClaudeFormatter::format(context) -> String
GPTFormatter::format(context) -> String
```

#### 3. Implementation Plan

**Phase 1: Basic Query** (Week 1)
- [ ] Add `context` command to CLI
- [ ] Implement keyword extraction
- [ ] Wire up existing KnowledgeStore queries
- [ ] Output raw results

**Phase 2: Smart Assembly** (Week 2)
- [ ] Build relevance ranking
- [ ] Add token budgeting
- [ ] Implement context expansion via call graph
- [ ] Handle multiple result aggregation

**Phase 3: LLM Integration** (Week 3)
- [ ] Create formatter trait
- [ ] Implement Claude/GPT/Gemini formatters
- [ ] Add prompt templates
- [ ] Support different output modes

## Language Support

| Language | Functions | Types | Docs | Call Graph |
|----------|-----------|-------|------|------------|
| Rust     | âœ… | âœ… | âœ… | âœ… |
| Go       | âœ… | âœ… | âœ… | âœ… |
| Python   | âœ… | âœ… | âœ… | âœ… |
| JavaScript/TypeScript | âœ… | âœ… | âœ… | âœ… |
| Solidity | âœ… | âœ… | âœ… | âœ… |

## Performance

- **Extraction**: ~1,000 files/sec
- **Database Size**: ~2MB per 1,000 files
- **Query Time**: < 10ms
- **Token Reduction**: 10-100x vs raw files

## Historical Note

This engine was refactored from a monolithic 1,827-line file into modular components. During the refactor, we discovered and fixed:
- 11x call graph duplication bug
- SQL injection vulnerabilities  
- Database initialization issues

The refactored version extracts 58% more functions and 11x more relationships than the original - this is a feature, not a bug. The original was missing significant amounts of code structure.

## Success Metrics

- âœ… Extract semantic information from 6 languages
- âœ… Store in queryable format with incremental updates
- âœ… Provide sub-10ms query performance
- ðŸš§ Answer natural language questions about code
- ðŸš§ Generate optimal LLM context under token budgets
- ðŸš§ Support cross-repository analysis