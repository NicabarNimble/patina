---
id: neuro-symbolic-hybrid-extraction
version: 1
status: draft
created_date: 2025-10-24
updated_date: 2025-10-24
oxidizer: nicabar
tags: [architecture, neuro-symbolic, coreml, prolog, hybrid-ai]
---

# Neuro-Symbolic Hybrid Extraction Architecture

**Split AI responsibilities for optimal cost, privacy, and quality**

## Core Insight

Don't use one AI approach for everything. Split the workload:

1. **Cloud LLM (Claude)** ‚Üí Rule Writer (one-time, sophisticated reasoning)
2. **CoreML (on-device)** ‚Üí Fact Extractor (ongoing, privacy-first)
3. **Scryer Prolog** ‚Üí Inference Engine (combines both)

## The Problem with Single-Model Approaches

### All Cloud LLM
- ‚ùå High cost: $17+ for 227 sessions + ongoing per-session costs
- ‚ùå Privacy: Session content sent to third-party
- ‚ùå Network dependency: Can't work offline
- ‚ùå Latency: API calls add overhead
- ‚úÖ Best quality reasoning

### All CoreML
- ‚ùå Can't write sophisticated logic
- ‚ùå Limited reasoning about patterns
- ‚ùå Requires training data
- ‚úÖ Privacy-first
- ‚úÖ Zero cost
- ‚úÖ Fast

### Hybrid (This Approach)
- ‚úÖ Claude writes rules: **$0.50 one-time**
- ‚úÖ CoreML extracts facts: **$0 forever**
- ‚úÖ Session content stays on-device
- ‚úÖ Sophisticated reasoning (Prolog rules)
- ‚úÖ Fast extraction (Neural Engine)
- ‚úÖ Best of both worlds

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CLOUD LLM (Claude) - The Rule Architect             ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ Task: Design inference logic                        ‚îÇ
‚îÇ Input: Sample sessions (10-20)                      ‚îÇ
‚îÇ Output: rules.pl (Prolog inference rules)           ‚îÇ
‚îÇ Frequency: One-time or monthly                      ‚îÇ
‚îÇ Cost: ~$0.50                                         ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ Example output:                                      ‚îÇ
‚îÇ   mature_pattern(P) :-                              ‚îÇ
‚îÇ       pattern_observed(S1, P, _),                   ‚îÇ
‚îÇ       pattern_observed(S2, P, _),                   ‚îÇ
‚îÇ       temporal_spread([S1,S2], Days),               ‚îÇ
‚îÇ       Days > 30.                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚Üì rules.pl
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Scryer Prolog  ‚îÇ
         ‚îÇ  (Inference)    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚Üì facts.pl
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COREML (On-device) - The Fact Miner                 ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ Task: Extract structured facts from markdown        ‚îÇ
‚îÇ Input: Session markdown files (227+)                ‚îÇ
‚îÇ Output: facts.pl (Prolog facts)                     ‚îÇ
‚îÇ Frequency: Every session                            ‚îÇ
‚îÇ Cost: $0                                             ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ Example output:                                      ‚îÇ
‚îÇ   session('20251024-175253', '2025-10-24', ...).   ‚îÇ
‚îÇ   pattern_observed('20251024-175253',               ‚îÇ
‚îÇ                     'neuro-symbolic', architecture).‚îÇ
‚îÇ   tech_used('20251024-175253', 'coreml', ...).     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Workflow

### Phase 1: Rule Generation (Infrequent)

```bash
# Claude analyzes sample sessions and designs rules
patina rules generate \
  --samples 10 \
  --domain "software-development-patterns" \
  --output layer/buckets/patina-dev/rules.pl
```

**What Claude receives:**
- 10 representative sessions (full markdown)
- Domain description
- Schema requirements

**What Claude generates:**
```prolog
% rules.pl - Generated by Claude

% Pattern evolution
mature_pattern(Pattern) :-
    pattern_observed(S1, Pattern, _),
    pattern_observed(S2, Pattern, _),
    pattern_observed(S3, Pattern, _),
    S1 \= S2, S2 \= S3, S1 \= S3,
    temporal_spread([S1, S2, S3], Days),
    Days > 30.

% Technology adoption signals
technology_gaining_traction(Tech) :-
    findall(S, tech_used(S, Tech, _), Sessions),
    length(Sessions, Count),
    Count >= 2,
    recent_usage(Sessions).

% Decision alignment with philosophy
philosophical_decision(SessionId, Choice) :-
    decision(SessionId, Choice, Rationale),
    (sub_string(Rationale, _, _, _, "privacy") ;
     sub_string(Rationale, _, _, _, "open source") ;
     sub_string(Rationale, _, _, _, "offline")).

% Helper predicates
temporal_spread(Sessions, Days) :-
    findall(Date, (member(S, Sessions), session(S, Date, _, _, _, _)), Dates),
    sort(Dates, Sorted),
    Sorted = [First|_],
    last(Sorted, Last),
    days_between(First, Last, Days).
```

### Phase 2: Fact Extraction (Ongoing)

```bash
# CoreML extracts facts from all sessions
patina session extract \
  --mode coreml \
  --all \
  --output layer/buckets/patina-dev/facts.pl
```

**CoreML process:**
1. Load MobileBERT QA model (97MB, runs on Neural Engine)
2. For each session markdown:
   - Ask: "What patterns were observed?"
   - Ask: "What technologies were used and why?"
   - Ask: "What decisions were made?"
   - Ask: "What problems were solved?"
3. Parse answers into Prolog facts
4. Append to facts.pl

**Performance:**
- 227 sessions √ó 20ms = ~5 seconds total
- Each new session: ~20ms
- Runs entirely on Neural Engine (battery efficient)

### Phase 3: Inference (Always)

```bash
# Query using Scryer Prolog
scryer-prolog facts.pl rules.pl

# Run Claude's sophisticated queries
?- mature_pattern(Pattern).
Pattern = 'neuro-symbolic-persona' ;
Pattern = 'domain-buckets'.

?- technology_gaining_traction(Tech).
Tech = 'scryer-prolog' ;
Tech = 'coreml'.

?- philosophical_decision(Session, Choice).
Session = '20251024-175253',
Choice = 'coreml-over-cloud-llm'.
```

## Technology Stack

### Cloud Layer (Rule Writing)
- **Claude Sonnet 4.5**: Best reasoning for Prolog rule design
- **Cost**: ~$0.50 for initial generation
- **Frequency**: Monthly or when domain evolves

### On-Device Layer (Fact Extraction)
- **MobileBERT-SQuAD** (97MB): Question answering, CoreML optimized
- **Apple NaturalLanguage**: Entity recognition (built-in)
- **Sentence-BERT** (80MB, optional): Semantic similarity
- **Total**: ~177MB models
- **Runtime**: Neural Engine + Metal acceleration

### Inference Layer
- **Scryer Prolog**: Rust-based Prolog (native ARM64)
- **SQLite**: Efficient fact storage (optional)
- **Integration**: Swift bridge for CoreML, Rust CLI

## Implementation

### Command: Generate Rules

```rust
// src/commands/rules/generate.rs
pub async fn generate_rules(
    sample_paths: &[PathBuf],
    domain: &str,
    output: &Path
) -> Result<()> {
    // 1. Read sample sessions
    let samples: Vec<String> = sample_paths
        .iter()
        .map(|p| fs::read_to_string(p))
        .collect::<Result<Vec<_>>>()?;

    // 2. Build prompt for Claude
    let prompt = format!(
        r#"You are a Prolog expert designing inference rules for a knowledge base.

Domain: {domain}

Sample sessions (showing pattern diversity):
{}

Design a Scryer Prolog rule system that:
1. Defines the fact schema (what should be extracted from each session)
2. Writes inference rules for:
   - Pattern evolution (emerging ‚Üí mature ‚Üí core promotion)
   - Technology adoption signals
   - Decision analysis (philosophical vs pragmatic)
   - Problem-solution pattern detection
   - Cross-session correlations
3. Includes helper predicates for temporal analysis, counting, etc.
4. Enables queries like "What patterns are ready for core?" and "What tech is gaining traction?"

Output only valid Scryer Prolog code.
Return the complete rules.pl file."#,
        samples.join("\n\n---\n\n")
    );

    // 3. Call Claude API
    let client = anthropic_client()?;
    let response = client.messages()
        .create(MessagesRequest {
            model: "claude-sonnet-4.5".into(),
            messages: vec![Message::user(prompt)],
            max_tokens: 4096,
        })
        .await?;

    // 4. Extract Prolog code
    let rules = extract_code_block(&response.content, "prolog")?;

    // 5. Validate syntax (compile check with Scryer)
    validate_prolog_syntax(&rules)?;

    // 6. Write to output
    fs::write(output, rules)?;

    println!("‚úì Generated rules.pl ({} lines)", rules.lines().count());

    Ok(())
}
```

### Command: Extract Facts (CoreML)

```swift
// resources/coreml-extractor/main.swift
import Foundation
import CoreML
import NaturalLanguage

struct SessionFacts {
    var sessionId: String
    var patterns: [Pattern]
    var technologies: [Technology]
    var decisions: [Decision]
    var challenges: [Challenge]
}

func extractSession(markdown: String) throws -> String {
    // Load CoreML model
    let qa = try MobileBERTSQuAD(configuration: MLModelConfiguration())

    // Extract metadata
    let sessionId = extractSessionId(from: markdown)
    let workType = ask(qa, question: "What type of work was this?", context: markdown)
    let branch = extractBranch(from: markdown)

    var prolog = ""

    // Session fact
    prolog += "session('\(sessionId)', '\(date)', \(workType), '\(branch)', \(commits), \(files)).\n"

    // Pattern observations
    let patternsAnswer = ask(qa,
        question: "What patterns were observed? List each pattern name and category.",
        context: markdown
    )
    for pattern in parsePatterns(patternsAnswer) {
        prolog += "pattern_observed('\(sessionId)', '\(pattern.name)', \(pattern.category)).\n"
    }

    // Technologies
    let techAnswer = ask(qa,
        question: "What technologies or tools were used and for what purpose?",
        context: markdown
    )
    for tech in parseTechnologies(techAnswer) {
        prolog += "tech_used('\(sessionId)', '\(tech.name)', '\(tech.purpose)').\n"
    }

    // Decisions
    let decisionsAnswer = ask(qa,
        question: "What key decisions were made and what was the rationale?",
        context: markdown
    )
    for decision in parseDecisions(decisionsAnswer) {
        prolog += "decision('\(sessionId)', '\(decision.choice)', '\(decision.rationale)').\n"
    }

    // Challenges
    let challengesAnswer = ask(qa,
        question: "What problems were encountered and how were they solved?",
        context: markdown
    )
    for challenge in parseChallenges(challengesAnswer) {
        prolog += "challenge('\(sessionId)', '\(challenge.problem)', '\(challenge.solution)').\n"
    }

    return prolog
}

func ask(_ model: MobileBERTSQuAD, question: String, context: String) -> String {
    let input = MobileBERTSQuADInput(question: question, context: context)
    guard let output = try? model.prediction(input: input) else {
        return ""
    }
    return output.answer
}

// Main entry point
let markdownPath = CommandLine.arguments[1]
let markdown = try String(contentsOfFile: markdownPath)
let facts = try extractSession(markdown: markdown)
print(facts)
```

**Rust integration:**

```rust
// src/commands/session/extract.rs
pub fn extract_session_coreml(session_path: &Path) -> Result<String> {
    let output = Command::new("./target/release/coreml-extractor")
        .arg(session_path)
        .output()
        .context("Failed to run CoreML extractor")?;

    if !output.status.success() {
        bail!("Extraction failed: {}", String::from_utf8_lossy(&output.stderr));
    }

    Ok(String::from_utf8(output.stdout)?)
}

pub fn extract_all_sessions(output_path: &Path) -> Result<()> {
    let sessions = glob("layer/sessions/*.md")?;
    let mut all_facts = String::new();

    all_facts.push_str("% Facts extracted from sessions\n");
    all_facts.push_str(&format!("% Generated: {}\n\n", chrono::Utc::now()));

    for session in sessions {
        let session_path = session?;
        println!("Extracting: {}", session_path.display());

        let facts = extract_session_coreml(&session_path)?;
        all_facts.push_str(&facts);
        all_facts.push('\n');
    }

    fs::write(output_path, all_facts)?;

    Ok(())
}
```

### Command: Query Knowledge Base

```rust
// src/commands/query.rs
use scryer_prolog::{Machine, QueryResult};

pub fn query_knowledge(query: &str, facts_path: &Path, rules_path: &Path) -> Result<()> {
    let mut machine = Machine::new();

    // Load facts and rules
    machine.consult(facts_path)?;
    machine.consult(rules_path)?;

    // Execute query
    let results = machine.run_query(query)?;

    // Display results
    for result in results {
        println!("{}", result);
    }

    Ok(())
}
```

## Cost & Performance Comparison

### Initial Extraction (227 sessions)

| Approach | Time | Cost | Privacy | Quality |
|----------|------|------|---------|---------|
| **Hybrid (This)** | 5 sec | $0.50 | ‚úÖ Facts private | ‚úÖ High (Claude rules) |
| All Cloud LLM | 2 min | $17.00 | ‚ùå All content exposed | ‚úÖ Highest |
| All CoreML | 5 sec | $0.00 | ‚úÖ Fully private | üü° Good extraction, basic inference |

### Ongoing (Per Session)

| Approach | Time | Cost | Privacy |
|----------|------|------|---------|
| **Hybrid** | 20ms | $0.00 | ‚úÖ Private |
| Cloud LLM | 500ms | $0.08 | ‚ùå Exposed |
| CoreML | 20ms | $0.00 | ‚úÖ Private |

## Rule Evolution

As your domain grows, Claude can evolve the rules:

```bash
# Initial rules
patina rules generate --version 1.0 --output rules.pl

# Later: Evolved rules based on new patterns
patina rules evolve \
  --current rules.pl \
  --feedback "Need to detect refactoring patterns" \
  --new-samples layer/sessions/2025*.md \
  --output rules-v1.1.pl
```

Claude updates with new inference logic while preserving existing rules.

## Benefits

### Privacy-First
- Session content (potentially sensitive) never leaves device
- Only anonymized sample sessions sent to Claude for rule design
- Facts stored locally in facts.db

### Cost-Effective
- One-time $0.50 for sophisticated rule design
- Zero ongoing cost for fact extraction
- vs $17+ for cloud LLM approach

### Apple Silicon Native
- Neural Engine acceleration (16-core on M3)
- Metal GPU fallback
- Unified memory (no copying between CPU/GPU)
- Battery efficient (10x more efficient than CPU)

### Quality Where It Matters
- Claude writes sophisticated Prolog logic (temporal reasoning, correlations)
- CoreML does reliable structured extraction
- Scryer Prolog provides powerful inference

### Offline-Capable
- CoreML runs without network
- Only rule generation requires cloud
- Can work completely disconnected after initial setup

## Future Enhancements

### Multi-Domain Support
```bash
# Generate domain-specific rules
patina rules generate --domain rust-development
patina rules generate --domain security-patterns
patina rules generate --domain devops

# Cross-domain queries
?- rust_pattern(P), security_concern(P, Concern).
```

### Active Learning
```bash
# CoreML flags low-confidence extractions
patina session extract --review-threshold 0.7

# Human reviews flagged items
patina session review

# Re-train with corrections (optional)
```

### Persona Integration
```prolog
% User preferences as facts (central ~/.patina/)
user_prefers(privacy, high).
user_prefers(cost, low).
user_prefers(stack, apple_silicon).

% Rules consider preferences
recommend_tool(Tool) :-
    tool_available(Tool),
    user_prefers(privacy, high),
    tool_privacy_level(Tool, high).
```

## Implementation Path

1. **Week 1**: Rule generation command
   - Integrate Anthropic API
   - Design prompt template
   - Generate initial rules.pl from samples

2. **Week 2**: CoreML extractor (Swift)
   - Download MobileBERT model
   - Build Swift extraction tool
   - Test on sample sessions

3. **Week 3**: Rust integration
   - Swift bridge for CoreML calls
   - Batch extraction command
   - Write to facts.pl

4. **Week 4**: Prolog inference
   - Integrate Scryer Prolog
   - Query command implementation
   - SQLite export (optional)

5. **Week 5**: Polish & docs
   - Error handling
   - Progress indicators
   - Documentation
   - Example queries

## Related Patterns

- `domain-architecture.md` - Self-contained domain buckets
- `pattern-selection-framework.md` - When to use which patterns
- `layer/buckets/patina-dev/DESIGN.md` - Original neuro-symbolic POC

## Philosophy Alignment

‚úÖ **Knowledge First**: Sophisticated rules + reliable extraction
‚úÖ **Privacy First**: On-device processing for sensitive data
‚úÖ **Cost Conscious**: One-time setup, zero ongoing
‚úÖ **Offline Capable**: Network only for rule generation
‚úÖ **Apple Silicon Native**: Full stack optimization
‚úÖ **Escape Hatches**: Can swap CoreML for cloud if needed
‚úÖ **Tool Composition**: Each component has clear input ‚Üí output
